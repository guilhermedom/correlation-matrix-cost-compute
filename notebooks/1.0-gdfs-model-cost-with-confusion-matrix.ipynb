{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqnYpHPuxN6V"
      },
      "source": [
        "# Evaluating Machine Learning Models with Confusion and Weighted Error Matrices\n",
        "\n",
        "Confusion matrices are often used to evaluate the performance of machine learning models. Model predictions resulting in true positives and negatives indicate how precise the model was. Meanwhile, false positives and negatives reveal the number of mistakes committed by the model.\n",
        "\n",
        "In most real scenarios, however, types of errors have different costs. Usually, when a model is evaluating if a pacient has a disease, if the model predicts incorrectly, it is better that the prediction ends up being a false positive than a false negative. It is better to have a healthy pacient getting treatment than to have a diseased patient not receiving any help. \n",
        "\n",
        "To regulate the cost of each type of error in a confusion matrix, we can combine it with a weighted error matrix. In the error matrix we can attribute a value for each cell that will be the weight for a specific error.\n",
        "\n",
        "In the next code cell we load up a dataset having weather information for Australia. The objective for models here is to predict if it is going to rain in the next day or not. Note that this is a modified version of the original \"weatherAUS\" dataset. In this version, there are no information about wind conditions, city names or dates. As the dataset columns carry distinct information (temperature, humidity) with varying intervals for values, we normalize the data before using it to train models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "SDvz_SBekBU4",
        "outputId": "991a7df2-d225-4671-a8a2-e7ab6bef61cc"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# read file and transform 'No' and 'Yes' values to 0 and 1, so that we can use them in classifiers\n",
        "with open('../data/raw/weatherAUS_modified.csv') as csv_file:\n",
        "    csv_file.readline()\n",
        "    dataset = [list(map(float, x.replace('\\n', '').replace('No', '0').replace('Yes', '1').split(','))) for x in csv_file] \n",
        "\n",
        "dataset = np.array(dataset)\n",
        "x = dataset[:,:-1]\n",
        "y = dataset[:,-1]\n",
        "\n",
        "# center mean and set unit standard deviation\n",
        "scaler = StandardScaler(with_mean=True, with_std=True)\n",
        "x = scaler.fit_transform(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Besides from evaluating a model, it would also be good to compare its performance with other models. Fortunately, the weighted error is even more useful for that. In this simple testbed, we are going to train a simple model, Naive Bayes, and a more complex one, Random Forest. We are going to use stratified cross-validation so that we can better evaluate models while mitigating random effects.\n",
        "\n",
        "After training and predicting with a model, we take the confusion matrix for the prediction result and multiply it by the weighted error matrix passed to the function. In this manner, we can attribute different weights to false positives and false negatives."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "id": "7o3cu6caoLtA",
        "outputId": "ca42a6bf-dd8c-4d18-d26e-9c293d8fba16"
      },
      "outputs": [],
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# train and test a given model using stratified cross-validation with 10 folds\n",
        "def test_model(model, _x, _y, n_splits=10):\n",
        "    skf_10 = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=0)\n",
        "    \n",
        "    # lists for each split model cost and number of FPs and FNs\n",
        "    cost_list = []\n",
        "    fp_list = []\n",
        "    fn_list = []\n",
        "    for train_ind, test_ind in skf_10.split(_x, _y):  \n",
        "        # train and test every model using a cross-validation data split\n",
        "        x_train, x_test = x[train_ind], x[test_ind]\n",
        "        y_train, y_test = y[train_ind], y[test_ind]\n",
        "        model.fit(x_train, y_train)\n",
        "        y_pred = model.predict(x_test)\n",
        "\n",
        "        # get the confunsion matrix for the model in this split\n",
        "        cm = confusion_matrix(y_test, y_pred)\n",
        "        \n",
        "        # calculate cost by weighing the confusion matrix with the error weight matrix\n",
        "        cost = (cm[0, 0] * error_cost[0, 0]) + (cm[0, 1] * error_cost[0, 1]) + (cm[1, 0] * error_cost[1, 0]) + (cm[1, 1] * error_cost[1, 1])\n",
        "        cost_list.append(cost)\n",
        "        fn_list.append(cm[1, 0])\n",
        "        fp_list.append(cm[0, 1])\n",
        "        \n",
        "    return cost_list, fn_list, fp_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For this weather prediction scenario, we follow the field standard to weigh errors. It is worse to say that it won't rain when it will because people need to prepare for rain. In the other case, when it does not rain, even if people prepared for rain, they can probably continue doing their things regardless.\n",
        "\n",
        "Therefore, we are going to attribute a weight of 1 for false positives of raining in the next day, and 5 for false negatives. With this error matrix, we can see that even an overall better model like Random Forest can be surpassed by the simpler Naive Bayes. As we can see, the Random Forest model makes less mistakes in general, but it commits more false negatives mistakes. In this scenario, when false negatives are worse, the weight of these errors make Random Forest have the worst performance. Conclusively, we can see that Naive Bayes has the smaller average cost."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Naive Bayes' average cost: 4218.900000\n",
            "Number of false negatives with Naive Bayes: 6725\n",
            "Number of false positives with Naive Bayes: 8564\n",
            "Random Forest's average cost: 4604.200000\n",
            "Number of false negatives with Random Forest: 8572\n",
            "Number of false positives with Random Forest: 3182\n"
          ]
        }
      ],
      "source": [
        "# error weight matrix with cost 1 for false positives, 5 for false negatives and 0 for TP/TN\n",
        "error_cost = np.array([[0, 1], [5, 0]])\n",
        "\n",
        "# instantiate and test model with default hyperparameters\n",
        "gnb = GaussianNB()\n",
        "nb_list = test_model(gnb, x, y)\n",
        "print('Naive Bayes\\' average cost: %f' % np.mean(nb_list[0]))\n",
        "print('Number of false negatives with Naive Bayes: %d' % sum(nb_list[1]))\n",
        "print('Number of false positives with Naive Bayes: %d' % sum(nb_list[2]))\n",
        "\n",
        "# instantiate and test model with default hyperparameters\n",
        "rf = RandomForestClassifier(random_state=0)\n",
        "rf_list = test_model(rf, x, y)\n",
        "print('Random Forest\\'s average cost: %f' % np.mean(rf_list[0]))\n",
        "print('Number of false negatives with Random Forest: %d' % sum(rf_list[1]))\n",
        "print('Number of false positives with Random Forest: %d' % sum(rf_list[2]))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Prova_2_GuilhermeDomingos_WallaceManzano.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.8.12 ('bf_minhashing')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "b39e83d7d937e7ce85387f84ef071986d0096be4afb1ae7c3fe160a424884580"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
